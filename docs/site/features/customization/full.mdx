---
title: Details
---

## Full Explanation of `config.yaml`
We use a single `config.yaml` file as the source to configure Memobase Backend. An example is like this:

```yaml
max_chat_blob_buffer_token_size: 1024
buffer_flush_interval: 3600

llm_api_key: YOUR-KEY
llm_base_url: https://api.openai.com/v1/

best_llm_model: gpt-4o

additional_user_profiles:
  - topic: "gaming"
    sub_topics:
      - "Soul-Like"
...
```



### Storage Config
- `max_chat_blob_buffer_token_size`: int, default to `1024`. This is the parameter to control the buffer size of Memobase. Large the number, lower your LLM cost will be, but more lagging of profile update.
- `max_pre_profile_token_size`: int, default to `512`. The maximum token size of one profile slot can be. When a profile slot is larger than this, it will be trigger a re-summary.
- `max_profile_subtopics`: int, default to `15`. The maximum subtopics of one topic can be. When a topic has more than this, it will be trigger a re-organization.
- `persistent_chat_blobs`: bool, default to `false`. If set to `true`, the chat blobs will be persisted in the database.

### Profile Config
Check what is profile in Memobase in [here](/features/customization/profile)
- `additional_user_profiles`: list, default to `[]`. This is the parameter to add additional user profiles. Each profile should have a `topic` and a list of `sub_topics`.
    - For `topic`, it must have a `topic` field and maybe a `description` field:
    ```yaml
    additional_user_profiles:
      - topic: "gaming"
        # description: "User's gaming interests"
        sub_topics:
          ...
    ```
    - For each `sub_topic`, it must have a `name` field(or just a `string`) and maybe a `description` field:
    ```yaml
    sub_topics:
      - "SP1"
      - name: "SP2"
        description: "Sub-Profile 2" 
    ```
- `overwrite_user_profiles`: list, default to `null`. The format is the same as `additional_user_profiles`.
Memobase will have some built-in profile slots like `work_title`, `name` etc. If you want full control of the slots, you can use this parameter.
The final profiles slots will be those you defined in here.

### LLM Config
- `language`: string, default to `en`, available options `{'en', 'zh'}`. The prompt language of Memobase you like to use.
- `llm_base_url`: string, default to `https://api.openai.com/v1/`. The base URL of any OpenAI-Compatible API.
- `llm_api_key`: string, default to `null`. Your LLM API key.
- `best_llm_model`: string, default to `gpt-4o-mini`. The AI model to use.

### Environment Variable Overrides

All configuration values can be overridden using environment variables. The naming convention is to prefix the configuration field name with `MEMOBASE_` and convert it to uppercase.

For example, to override the `llm_api_key` configuration:

```bash
export MEMOBASE_LLM_API_KEY="your-api-key-here"
```

This is particularly useful for:
- Keeping sensitive information like API keys out of configuration files
- Deploying to different environments (development, staging, production)
- Containerized deployments where environment variables are the preferred configuration method

For complex data types (lists, dictionaries, etc.), you can use JSON-formatted strings:

```bash
# Override additional_user_profiles with a JSON array
export MEMOBASE_ADDITIONAL_USER_PROFILES='[{"topic": "gaming", "sub_topics": ["RPG", "Strategy"]}]'
```

The server will automatically parse JSON-formatted environment variables when appropriate.
